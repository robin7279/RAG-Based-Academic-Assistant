# RAG-Based-Academic-Assistant
ğŸ“ A Retrieval-Augmented Generation (RAG) based academic assistant that lets you upload PDFs, ask questions, and get smart answers from your documents.
It built with **Streamlit**, **FastAPI**, and **LangGraph**, powered by **local document processing** and conversational memory.

## ğŸš€ Features

- ğŸ’¬ Conversational AI powered by LangGraph
- ğŸ“„ Upload and index PDF academic documents
- ğŸ” Contextual Q&A from uploaded files
- ğŸ“ Attachment-style file upload next to chat input
- ğŸ§  Memory support per conversation thread

## ğŸ› ï¸ Requirements

Make sure you have the following in your `requirements.txt`:


## ğŸ“‚ Folder Structure

â”œâ”€â”€ main.py # Main application file 
â”œâ”€â”€ llm_handler.py # LangGraph configuration and LLM setup 
â”œâ”€â”€ knowledge_base.py # Create embeddings and vectorstore 
â”œâ”€â”€ books/  # PDF data 
â”œâ”€â”€ vectorstore/ # Saved FAISS vectorstore 
â”œâ”€â”€ .env  # For environment variable for Groq API
â””â”€â”€requirements.txt 


## âš™ï¸ Usage

1. **Install dependencies**  
   ```bash
   pip install -r requirements.txt

2. **Run the app**  
   ```bash
   streamlit run main.py
